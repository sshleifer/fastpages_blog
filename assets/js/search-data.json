{
  
    
        "post0": {
            "title": "Title",
            "content": "cut . In the code all this ignoring of future words is done with attention masks: tensors of shape (batch_size, num_heads, seq_len, seq_len). If we just consider the last two dimensions, entry i,j indicates whether the output for token i should consider the layer output from token j . .",
            "url": "https://sshleifer.github.io/fastpages_blog/2020/03/07/Untitled.html",
            "relUrl": "/2020/03/07/Untitled.html",
            "date": " • Mar 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Bart WalkThrough",
            "content": "Seq2Seq Pretraining . In October 2019, teams from Microsoft, Google and Facebook independently published three new transformer papers: UniLM, T5 and Bart. All three papers analyze found, in general that they achieve better downstream performance if they (a) replace Bert&#39;s bidirectional architecture with a seq2seq architecture and (b) Bert&#39;s fill-in-the blank cloze task with a more complicated mix of pretraining tasks. It&#39;s a fun game to try to match which of the following quotes from the abstract map to which paper: . While many modern approaches to transfer learning for NLP use a Transformer architecture consisting of only a single “stack” (e.g. for language modeling [GPT2] or classification and span prediction [BERT]), we found that using a standard encoder-decoder structure achieved good results on both generative and classification tasks. . | The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. . | We present a denoising autoencoder for pretraining sequence to sequence models, ... it uses a standard Transformer-based neural machine translation architecture. . | Answers: [^1]. [^1]: *Answers: (T5, Oct 24) , (UniLM, Oct. 15) , (Bart, Oct. 29) . If you got that right you are very lucky indeed! . So lets dig a bit deeper on this big Seq2Seq idea, then dive into some interesting parts of the code! . Big Idea: Bidirectional Encoder, Left-To-Right Decoder . Bert is pretrained to try to predict mask tokens, and uses the whole sequence to get enough info to make a good guess. This is good for tasks where the prediction at position i can depend on information from positions after i, but suboptimal for tasks where you are not, like text generation, where you generate the next word conditional on the words you have seen BEFORE. . In the code, bert&#39;s &quot;Fully Visible&quot; attention_mask is boring: . . GPT2, meanwhile, is pretrained to predict the next word. This makes it good at generation tasks, where there aren&#39;t future tokens to consider, but suboptimal for other downstream tasks where the causal mask provides no benefit. . Here is the attention mask for GPT2, white squares in y2, x3 show that the prediction for timestep 2 does not depend on the input at timestep 3. . . Our new friends get the best of both worlds: . The encoder is bidirectional  - each token&#39;s attention can attend to every other token in the input sequence, while the decoder, which will ultimately have to perform generation, is causal like GPT2. . We can think about this attention pattern as smushing together our previous two attention masks, or &quot;Causal Mask with a fully visible prefix&quot; in fancier terms.1 . . The indices dont line up perfectly for the smush to work, but tokens 1 and 2 are the fully visible prefix (or the input to the encoder) and tokens 3,4,5 are the causally masked suffix (or inputs to the decoder). In summarization terms, you could imagine tokens 1 and 2 as the article, and we generate tokens 3-5 auto-regressively.&#8617; . | . This attention pattern is very well suited to summarization and other conditional generation tasks. You are allowed to attend to the whole document, but as you write your summary, one word at a time, you need only consider what you&#39;ve already written. The numbers confirm this: all the new fancy guys do a lot better than the old less-fancy guys. . #collapse-hide import pandas as pd pd.read_csv(&#39;tab1.csv&#39;, index_col=0) . . CNNDM Rouge 2 score . Paper . Bart 21.28 | . UniLM 20.30 | . BertSumABS 19.39 | . t5-base 20.34 | . t5 11B 21.55 | . TransformerAbs (2018) 17.76 | . BertSumABS [^2] , exploits the Seq2Seq architecture but doesn&#39;t pretrain the decoder. Also note that t5-11b is 22x bigger than Bart), and pretraining objectives. . Bart tries out crazy pretraining tasks that you can only do with a seq2seq architecture. Since &quot;Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.&quot; They invent a pretraining task called Text Infilling, where you replace a span of text with a single mask token. This span can be of any length, so the model also must learn how many tokens to generate. . There is also another trick in Bart: each decoder layer performs cross-attention over the final hidden state of the encoder output. This presumably nudges Bart towards generating summaries that are closely connected to the original (encoded) text. . Awkward Transition to Eng . Shortly after these papers were released our transformers users started asking for us to make them available in the repo, especially Bart. And now, a few months later, it&#39;s demo time! . Demo of transformers.BartForConditionalGeneration . imports . #collapse-hide import torch torch_device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; LONG_ARTICLE = &#39; (CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC &#39;s founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed &quot;in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.&quot; Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians &#39; efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday &#39;s ceremony, said it was a move toward greater justice. &quot;As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,&quot; he said, according to an ICC news release. &quot;Indeed, today brings us closer to our shared goals of justice and peace.&quot; Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. &quot;As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,&quot; she said. Rights group Human Rights Watch welcomed the development. &quot;Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court &#39;s treaty should speak out to welcome its membership,&quot; said Balkees Jarrah, international justice counsel for the group. &quot;What &#39;s objectionable is the attempts to undermine international justice, not Palestine &#39;s decision to join a treaty to which over 100 countries around the world are members.&quot; In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it &quot;strongly&quot; disagreed with the court &#39;s decision. &quot;As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,&quot; the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. &quot;We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,&quot; it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as &quot;Palestine.&quot; While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would &quot;conduct its analysis in full independence and impartiality.&quot; The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN &#39;s Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.&#39; LONG_ARTICLE . . &#39; (CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC &#39;s founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed &#34;in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.&#34; Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians &#39; efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday &#39;s ceremony, said it was a move toward greater justice. &#34;As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,&#34; he said, according to an ICC news release. &#34;Indeed, today brings us closer to our shared goals of justice and peace.&#34; Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. &#34;As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,&#34; she said. Rights group Human Rights Watch welcomed the development. &#34;Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court &#39;s treaty should speak out to welcome its membership,&#34; said Balkees Jarrah, international justice counsel for the group. &#34;What &#39;s objectionable is the attempts to undermine international justice, not Palestine &#39;s decision to join a treaty to which over 100 countries around the world are members.&#34; In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it &#34;strongly&#34; disagreed with the court &#39;s decision. &#34;As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,&#34; the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. &#34;We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,&#34; it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as &#34;Palestine.&#34; While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would &#34;conduct its analysis in full independence and impartiality.&#34; The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN &#39;s Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.&#39; . tokenizer = BartTokenizer.from_pretrained(&#39;bart-large-cnn&#39;) article_input_ids = tokenizer.batch_encode_plus([LONG_ARTICLE], return_tensors=&#39;pt&#39;)[&#39;input_ids&#39;].to(torch_device) model = BartForConditionalGeneration.from_pretrained(&#39;bart-large-cnn&#39;) summary_ids = model.generate(article_input_ids, num_beams=4, length_penalty=2.0, max_length=140, min_len=55) tokenizer.decode(summary_ids.squeeze(), ) . &#34;&lt;s&gt;The Palestinian Authority becomes the 123rd member of the International Criminal Court. The move gives the court jurisdiction over alleged crimes in Palestinian territories. Israel and the United States opposed the Palestinians&#39; efforts to join the body. But Palestinian Foreign Minister Riad al-Malki said it was a move toward greater justice.&#34; . TODO(SS): output of above gets smushed into one line it is really . TODO: get conditional generation working on GPT2 for this Doc. The following code just generates eos. . from transformers import GPT2LMHeadModel, GPT2Tokenizer gpt2_tok = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;) gpt2_model = GPT2LMHeadModel.from_pretrained(&#39;gpt2&#39;) article_input_ids = gpt2_tok.batch_encode_plus([LONG_ARTICLE], return_tensors=&#39;pt&#39;, pad_to_max_length=False)[&#39;input_ids&#39;].to(torch_device) summary_ids = gpt2_model.generate(article_input_ids, max_length=article_input_ids.shape[1] + 155, do_sample=False) gpt2_tok.decode(summary_ids.squeeze(), ).split(&#39; n&#39;) . One thing to notice in these two snippets: even though BartForConditionalGeneration is a seq2seq model, and GPT2LMHeadModel is not, they can invoked in similar ways for generation. . . Note: The same correspondence exists between BartForSequenceClassification and all the other *ForSequenceClassification in transformers. . BartModel (and all it&#39;s children)&#39;s full signature is a little more complex: . def forward( self, input_ids, attention_mask=None, # ignored pad tokens in input_ids decoder_input_ids=None, # make these if not supplied decoder_padding_mask=None, # ignored pad tokens in decoder_input_ids ): . When we&#39;re doing summarization finetuning, or seq2seq pretraining, we need to take in the second sequence. When we&#39;re not, like in a classification context, you can safely ignore all the decoder kwargs and BartModel will make them for you by taking the input_ids (movie review) and shifting them to the right. . The authors&#39; motivation for that trick was to facilitate teacher forcing during pre-training, and now that the model has been trained on 64 TPUs for 12 weeks to process this input format, we continue the pattern during inference, but hide it inside the forward method. . Incremental Decoding . When I first read the fairseq code, there was a function called make_generation_fast which didnt do much besides catch my eye. What an exciting name! Anyways, here is a really slow (pseudocode) way to greedily generate summaries . output_tokens = [] while not done: encoder_hidden_state = model.encoder(article_input_ids) logits = model.decoder(encoder_hidden_state, output_tokens) next_word = logits.argmax() output_tokens.append(next_word) if next_word == eos: break . We can just cache the first step and save half the compute . output_tokens = [] encoder_hidden_state = model.encoder(article_input_ids) while not done: logits = model.decoder(encoder_hidden_state, output_tokens) next_word = logits.argmax() output_tokens.append(next_word) if next_word == eos: break . Easy peasy, sorry for wasting your time. Here comes the fun one . Partially caching k and v in DecoderLayer . Here is some pseudocode for attention without all the reshapes and heads and masks and scaling. . class SimplifiedAttention(nn.Module): def __init__(self, embed_dim): self.Wq = torch.nn.Linear(embed_dim, embed_dim) self.Wk = torch.nn.Linear(embed_dim, embed_dim) self.Wv = torch.nn.Linear(embed_dim, embed_dim) self.dense = torch.nn.Linear(embed_dim, embed_dim) def forward(self, query, key, value): q = self.Wq(q) k = self.Wk(k) v = self.Wv(v) matmul_qk = torch.matmul(q, k.T) attention_weights = matmul_qk.softmax(dim=-1) output = torch.matmul(attention_weights, v) return self.dense(output) . Now lets glimpse at the callers inside bart&#39;s DecoderLayer: (LayerNorms and dropouts deleted for simplicity). Here&#39;s some more pseudocode . class SimplifiedDecoderLayer(nn.Module): def __init__(self, embed_dim): self.self_attn = SimplifiedAttention(embed_dim) self.encoder_attn = SimplifiedAttention(embed_dim) def forward(x, last_encoder_hidden_state, *masks_etc): # x shape `(batch_size, tokens_generated_so_far, embed_dim)` # x comes from decoder x = self.self_attn(query=x, key=x, value=x) # pay attention to somebody else for a change! output = self.encoder_attn( query=x, key=last_encoder_hidden_state, # could be None value=last_encoder_hidden_state, ) return output . What did we learn? . In encoder_attention, we can cache everything that doesn&#39;t depend on q, namely these outputs k = self.Wk(k) v = self.Wv(v) . | . The more exciting optimization is that in self_attn, we can cache the part of k,v that depends on x[:, :1] the tokens we&#39;ve already generated. Then each time through the generation loop, we only pass in x[:, :-1] and apply concatenation: . k = torch.cat((past_key, new_k), dim=&#39;seq_len&#39;) # the seq_len dimension, v = torch.cat((past_value, new_v), dim=&#39;seq_len&#39;) . TODO(SS): Why cant we cache part of q? . Of the 8 F.linear ops performed by each DecoderLayer was doing, we&#39;ve managed to completely cache 2 of them, and almost completely cache 2 more. Overall, we chop off about 40% of the runtime. TODO(SS): verify. . Conclusion . Our first release of BartModel prioritized moving quickly and keeping the code simple. As a result, our implementation is about 30 % slower and uses more memory than the authors&#39;. Stay tuned for episode 2 of this series, where we try to close the gap. . Footnotes . [^2] &quot;Text Summarization with Pretrained Encoders&quot; https://arxiv.org/abs/1908.08345 . [^3] Differences between the UniLM Masking strategy and Bart . Appendix . &quot;Left-to-right mask. For a given token in the sequence, we assign a mask value of 0 for this token and the preceding ones; a value of minus infinity for the later ones. This pretty square can also be called and causal_mask autoregressive mask. (Picture Drawn by Remy Louf)&quot; . Note Most of our other models do not make inputs for the user -- that&#39;s the tokenizer&#39;s job, but as the t5 authors write: &gt; &quot;A major factor that differentiates the architectures is the mask used by different attention mechanisms in the model.&quot; .",
            "url": "https://sshleifer.github.io/fastpages_blog/jupyter/2020/03/06/bart.html",
            "relUrl": "/jupyter/2020/03/06/bart.html",
            "date": " • Mar 6, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . # captions . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . | %%writefile tab1.csv Paper, CNNDM Rouge 2 score Bart, 21.28 UniLM, 20.3 BertSumABS, 19.39 t5-base, 20.34 t5 11B, 21.55 TransformerAbs (2018), 17.76 . Writing tab1.csv .",
            "url": "https://sshleifer.github.io/fastpages_blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Test Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://sshleifer.github.io/fastpages_blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sshleifer.github.io/fastpages_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}