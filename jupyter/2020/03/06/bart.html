<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Bart WalkThrough | fastpages</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Bart WalkThrough" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A journey through the forward pass" />
<meta property="og:description" content="A journey through the forward pass" />
<link rel="canonical" href="https://sshleifer.github.io/fastpages_blog/jupyter/2020/03/06/bart.html" />
<meta property="og:url" content="https://sshleifer.github.io/fastpages_blog/jupyter/2020/03/06/bart.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:image" content="https://sshleifer.github.io/fastpages_blog/images/chart-preview.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-06T00:00:00-06:00" />
<script type="application/ld+json">
{"datePublished":"2020-03-06T00:00:00-06:00","headline":"Bart WalkThrough","image":"https://sshleifer.github.io/fastpages_blog/images/chart-preview.png","description":"A journey through the forward pass","mainEntityOfPage":{"@type":"WebPage","@id":"https://sshleifer.github.io/fastpages_blog/jupyter/2020/03/06/bart.html"},"@type":"BlogPosting","url":"https://sshleifer.github.io/fastpages_blog/jupyter/2020/03/06/bart.html","dateModified":"2020-03-06T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/fastpages_blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://sshleifer.github.io/fastpages_blog/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/fastpages_blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Bart WalkThrough | fastpages</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Bart WalkThrough" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A journey through the forward pass" />
<meta property="og:description" content="A journey through the forward pass" />
<link rel="canonical" href="https://sshleifer.github.io/fastpages_blog/jupyter/2020/03/06/bart.html" />
<meta property="og:url" content="https://sshleifer.github.io/fastpages_blog/jupyter/2020/03/06/bart.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:image" content="https://sshleifer.github.io/fastpages_blog/images/chart-preview.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-06T00:00:00-06:00" />
<script type="application/ld+json">
{"datePublished":"2020-03-06T00:00:00-06:00","headline":"Bart WalkThrough","image":"https://sshleifer.github.io/fastpages_blog/images/chart-preview.png","description":"A journey through the forward pass","mainEntityOfPage":{"@type":"WebPage","@id":"https://sshleifer.github.io/fastpages_blog/jupyter/2020/03/06/bart.html"},"@type":"BlogPosting","url":"https://sshleifer.github.io/fastpages_blog/jupyter/2020/03/06/bart.html","dateModified":"2020-03-06T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://sshleifer.github.io/fastpages_blog/feed.xml" title="fastpages" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    // remove paragraph tags in rendered toc (happens from notebooks)
    var toctags = document.querySelectorAll(".toc-entry")
    toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/fastpages_blog/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/fastpages_blog/about/">About Me</a><a class="page-link" href="/fastpages_blog/search/">Search</a><a class="page-link" href="/fastpages_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Bart WalkThrough</h1><p class="page-description">A journey through the forward pass</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-03-06T00:00:00-06:00" itemprop="datePublished">
        Mar 6, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      15 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/fastpages_blog/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">
<a href="https://github.com/sshleifer/fastpages_blog/tree/master/_notebooks/2020-03-06-bart.ipynb" role="button">
    <img class="notebook-badge-image" src="/fastpages_blog/assets/badges/github.svg" alt="View On GitHub">
</a>
</div><div class="px-2">
    <a href="https://colab.research.google.com/github/sshleifer/fastpages_blog/blob/master/_notebooks/2020-03-06-bart.ipynb">
        <img class="notebook-badge-image" src="/fastpages_blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#Seq2Seq-Pretraining">Seq2Seq Pretraining </a></li>
<li class="toc-entry toc-h3"><a href="#Big-Idea:-Bidirectional-Encoder,-Left-To-Right-Decoder">Big Idea: Bidirectional Encoder, Left-To-Right Decoder </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Awkward-Transition-to-Eng">Awkward Transition to Eng </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#transformers.BartForConditionalGeneration">transformers.BartForConditionalGeneration </a>
<ul>
<li class="toc-entry toc-h4"><a href="#imports">imports </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Trick-2:-super_fast_cached_generation_mode">Trick 2: super_fast_cached_generation_mode </a></li>
<li class="toc-entry toc-h2"><a href="#Footnotes">Footnotes </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Appendix">Appendix </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Boxes-/-Callouts">Boxes / Callouts </a></li>
<li class="toc-entry toc-h2"><a href="#Footnotes">Footnotes </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-03-06-bart.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Seq2Seq-Pretraining">
<a class="anchor" href="#Seq2Seq-Pretraining" aria-hidden="true"><span class="octicon octicon-link"></span></a>Seq2Seq Pretraining<a class="anchor-link" href="#Seq2Seq-Pretraining"> </a>
</h3>
<p>In October 2019, teams from Microsoft, Google and Facebook independently published three new transformer papers: UniLM, T5 and Bart. All three papers analyze found, in general that they achieve better downstream performance if they (a) replace Bert's bidirectional architecture with a seq2seq architecture and (b) Bert's fill-in-the blank cloze task with a more complicated mix of pretraining tasks. It's a fun game to try to match which of the following quotes from the abstract map to which paper:</p>
<ol>
<li>
<blockquote>
<p>While many modern approaches to transfer learning for NLP use a Transformer architecture consisting of only a single “stack” (e.g. for language modeling [GPT2]  or classification and span prediction [BERT]), we found that using a standard encoder-decoder structure achieved good results on both generative and classification tasks.</p>
</blockquote>
</li>
<li>
<blockquote>
<p>The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction.</p>
</blockquote>
</li>
<li>
<blockquote>
<p>We present a denoising autoencoder for pretraining sequence to sequence models, ... it uses a standard Transformer-based neural machine translation architecture.</p>
</blockquote>
</li>
</ol>
<p>Answers: [^1].
 [^1]: *Answers: (T5, Oct 24) , (UniLM, Oct. 15) , (Bart, Oct. 29)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you got that right you are very lucky indeed!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So lets dig a bit deeper on this big Seq2Seq idea, then dive into some interesting parts of the code!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Big-Idea:-Bidirectional-Encoder,-Left-To-Right-Decoder">
<a class="anchor" href="#Big-Idea:-Bidirectional-Encoder,-Left-To-Right-Decoder" aria-hidden="true"><span class="octicon octicon-link"></span></a>Big Idea: Bidirectional Encoder, Left-To-Right Decoder<a class="anchor-link" href="#Big-Idea:-Bidirectional-Encoder,-Left-To-Right-Decoder"> </a>
</h3>
<p>Bert is pretrained to try to predict mask tokens, and uses the whole sequence to get enough info to make a good guess. This is good for tasks where the prediction at position <code>i</code> can depend on information from positions after <code>i</code>, but suboptimal for tasks where you are not, like text generation, where you generate the next word conditional on the words you have seen BEFORE.</p>
<p>In the code, bert's "Fully Visible" <code>attention_mask</code> is boring:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%html</span> 

#hide-input
{% include image.html alt="Drawing" style="width: 200px;" file="/fastpages_blog/images/copied_from_nb/bert_attn_mask.jpg" %}
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
#hide-input
<img src="bert_attn_mask.jpg" alt="Drawing" style="width: 200px;">

</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>GPT2, meanwhile, is pretrained to predict the next word.
This makes it good at generation tasks, where there aren't future tokens to consider, but suboptimal for other downstream tasks where the causal mask provides no benefit.</p>
<p>Here is the attention mask for GPT2, white squares in y2, x3 show that the prediction for timestep 2 does not depend on the input at timestep 3.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%html</span> 

#hide-input
{% include image.html alt="Drawing" style="width: 200px;" file="/fastpages_blog/images/copied_from_nb/gpt2_simple_mask.jpg" %}
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

#hide-input
<img src="gpt2_simple_mask.jpg" alt="Drawing" style="width: 200px;">

</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%html</span> 

#hide-input
{% include image.html alt="Drawing" style="width: 200px;" file="/fastpages_blog/images/copied_from_nb/gpt2_simple_mask.jpg" %}
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

#hide-input
<img src="gpt2_simple_mask.jpg" alt="Drawing" style="width: 200px;">

</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our new friends get the best of both worlds:</p>
<p>The encoder is bidirectional  - each token's attention can attend to every other token in the input sequence, while the decoder, which will ultimately have to perform generation, is causal like GPT2.</p>
<p>We can think about this attention pattern as smushing together our previous two attention masks, or "Causal Mask  with a fully visible prefix" in fancier terms.<sup class="footnote-ref" id="fnref-4"><a href="#fn-4">1</a></sup></p>
<div class="footnotes">
<hr>
<ol>
<li id="fn-4"><p>The indices dont line up perfectly for the smush to work, but tokens 1 and 2 are the fully visible prefix (or the input to the encoder) and tokens 3,4,5 are the causally masked suffix (or inputs to the decoder). In summarization terms, you could imagine tokens 1 and 2 as the article, and we generate tokens 3-5 auto-regressively.<a href="#fnref-4" class="footnote">↩</a></p></li>
</ol>
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/fastpages_blog/images/copied_from_nb/./t5_mask_diagram.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This attention pattern is very well suited to summarization and other conditional generation tasks.  You are allowed to attend to the whole document, but as you write your summary, one word at a time, you need only consider what you've already written. The numbers confirm this: all the new fancy guys do a lot better than the old less-fancy guys.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide </span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'tab1.csv'</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CNNDM Rouge 2 score</th>
    </tr>
    <tr>
      <th>Paper</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Bart</th>
      <td>21.28</td>
    </tr>
    <tr>
      <th>UniLM</th>
      <td>20.30</td>
    </tr>
    <tr>
      <th>BertSumABS</th>
      <td>19.39</td>
    </tr>
    <tr>
      <th>t5-base</th>
      <td>20.34</td>
    </tr>
    <tr>
      <th>t5 11B</th>
      <td>21.55</td>
    </tr>
    <tr>
      <th>TransformerAbs (2018)</th>
      <td>17.76</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<!-- I start with this not to say that the researchers are lame, but that they were all discovering a very cool idea at the same time. In the following table, you can see that, at least for summarization, it is important to pretrain with a bidirectional encoder and causal decoder: (The UniLM `prefix_lm` is equivalent [^3]) -->

<p><code>BertSumABS</code> [^2] , exploits the Seq2Seq architecture but doesn't pretrain the decoder.
Also note that t5-11b is 22x bigger than Bart), and pretraining objectives.</p>
<p>Bart tries out crazy pretraining tasks that you can only do with a seq2seq architecture. Since "Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations." They invent a pretraining task called Text Infilling, where you
replace a <strong>span</strong> of text with a single mask token. This span can be of any length, so the model also must learn how many tokens to generate.</p>
<p>There is also another trick in Bart: each decoder layer performs cross-attention over the final hidden state of the encoder output. This presumably nudges Bart towards generating summaries that are closely connected to the original (encoded) text.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Awkward-Transition-to-Eng">
<a class="anchor" href="#Awkward-Transition-to-Eng" aria-hidden="true"><span class="octicon octicon-link"></span></a>Awkward Transition to Eng<a class="anchor-link" href="#Awkward-Transition-to-Eng"> </a>
</h4>
<p>Shortly after these papers were released our <code>transformers</code> users started asking for us to make them available in the repo, especially Bart. And now, a few months later, it's demo time!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="transformers.BartForConditionalGeneration">
<a class="anchor" href="#transformers.BartForConditionalGeneration" aria-hidden="true"><span class="octicon octicon-link"></span></a><code>transformers.BartForConditionalGeneration</code><a class="anchor-link" href="#transformers.BartForConditionalGeneration"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="imports">
<a class="anchor" href="#imports" aria-hidden="true"><span class="octicon octicon-link"></span></a>imports<a class="anchor-link" href="#imports"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch_device</span> <span class="o">=</span> <span class="s1">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">'cpu'</span>
<span class="n">LONG_ARTICLE</span> <span class="o">=</span> <span class="s1">' (CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC</span><span class="se">\'</span><span class="s1">s founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed "in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014." Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians</span><span class="se">\'</span><span class="s1"> efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday</span><span class="se">\'</span><span class="s1">s ceremony, said it was a move toward greater justice. "As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice," he said, according to an ICC news release. "Indeed, today brings us closer to our shared goals of justice and peace." Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. "As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly," she said. Rights group Human Rights Watch welcomed the development. "Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court</span><span class="se">\'</span><span class="s1">s treaty should speak out to welcome its membership," said Balkees Jarrah, international justice counsel for the group. "What</span><span class="se">\'</span><span class="s1">s objectionable is the attempts to undermine international justice, not Palestine</span><span class="se">\'</span><span class="s1">s decision to join a treaty to which over 100 countries around the world are members." In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it "strongly" disagreed with the court</span><span class="se">\'</span><span class="s1">s decision. "As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC," the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. "We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace," it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as "Palestine." While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would "conduct its analysis in full independence and impartiality." The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN</span><span class="se">\'</span><span class="s1">s Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.'</span> 
<span class="n">LONG_ARTICLE</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>' (CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC\'s founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed "in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014." Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians\' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday\'s ceremony, said it was a move toward greater justice. "As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice," he said, according to an ICC news release. "Indeed, today brings us closer to our shared goals of justice and peace." Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. "As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly," she said. Rights group Human Rights Watch welcomed the development. "Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court\'s treaty should speak out to welcome its membership," said Balkees Jarrah, international justice counsel for the group. "What\'s objectionable is the attempts to undermine international justice, not Palestine\'s decision to join a treaty to which over 100 countries around the world are members." In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it "strongly" disagreed with the court\'s decision. "As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC," the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. "We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace," it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as "Palestine." While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would "conduct its analysis in full independence and impartiality." The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN\'s Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.'</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BartTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bart-large-cnn'</span><span class="p">)</span>
<span class="n">article_input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_encode_plus</span><span class="p">([</span><span class="n">LONG_ARTICLE</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'pt'</span><span class="p">)[</span><span class="s1">'input_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch_device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BartForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bart-large-cnn'</span><span class="p">)</span>
<span class="n">summary_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">article_input_ids</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">length_penalty</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                             <span class="n">max_length</span><span class="o">=</span><span class="mi">140</span><span class="p">,</span> <span class="n">min_len</span><span class="o">=</span><span class="mi">55</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">summary_ids</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>"&lt;s&gt;The Palestinian Authority becomes the 123rd member of the International Criminal Court. The move gives the court jurisdiction over alleged crimes in Palestinian territories. Israel and the United States opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki said it was a move toward greater justice."</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>TODO</strong>: get conditional generation working on GPT2 for this Doc. The following snippet goes straight to EOS.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2Tokenizer</span>
<span class="n">gpt2_tok</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'gpt2'</span><span class="p">)</span>
<span class="n">gpt2_model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'gpt2'</span><span class="p">)</span>

<span class="n">article_input_ids</span> <span class="o">=</span> <span class="n">gpt2_tok</span><span class="o">.</span><span class="n">batch_encode_plus</span><span class="p">([</span><span class="n">LONG_ARTICLE</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'pt'</span><span class="p">,</span> <span class="n">pad_to_max_length</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="s1">'input_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch_device</span><span class="p">)</span>
<span class="n">summary_ids</span> <span class="o">=</span> <span class="n">gpt2_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">article_input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">article_input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">155</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">gpt2_tok</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">summary_ids</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One thing to notice in these two snippets: even though <code>BartForConditionalGeneration</code> is a seq2seq model, and <code>GPT2LMHeadModel</code> is not, they can be used in very similar ways for generation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The same correspondence exists between <code>BartForSequenceClassification</code>  and <code>BertForSequenceClassification</code>.</p>
<p>It makes sense that you would want an Encoder and a Decoder if you wanted the number of stuffs you generate not to depend on the number of stuffs you take as input. You could even understand, in that world, having an API that was like</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
            <span class="n">encoder_input_ids</span><span class="p">,</span> 
            <span class="n">encoder_padding_mask</span><span class="p">,</span> 
            <span class="n">decoder_input_ids</span><span class="p">,</span> 
            <span class="n">decoder_causal_attention_mask</span><span class="p">,</span>
            <span class="n">decoder_padding_mask</span><span class="p">):</span>
</pre></div>
<p>But if you live in a world, like many of our users, where you expect your model to tell you the sentiment of a movie review, or the most likely next word, you would be like WTF is a decoder and why do I care: I just want to pass in a movie review my long winded friend Tim texted me and get a number so that I can decide whether to watch <em>The Notebook</em>.</p>
<p>I want a signature like</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">,</span>
    <span class="n">encoder_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">decoder_input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">decoder_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="p">):</span>
</pre></div>
<p>The way Bart makes decoder_input_ids, in this case, is by taking the input_ids (movie review) and shifting them to the right. The authors' <a href="https://github.com/pytorch/fairseq/issues/1389">motivation</a> for that trick was to facilitate teacher forcing during pre-training, and now that the model has been trained on 64 TPUs for 12 weeks to process this input format, we continue the pattern during inference, but hide it inside the forward method. We also hide the creation of the other decoder padding masks so that at least we are consistent within one signature (rewrite at least last sentence) [^1]
Now let's move back to our seq2seq world, so that we can generate a shorter version of Tim's movie review and make him think we read his review, watched The Notebook, and have a lot in common.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>Note 
Most of our other models do not make inputs for the user -- that's the tokenizer's job, but as the t5 authors write: &gt; "A major factor that differentiates the architectures is the mask used by different attention mechanisms in the model."</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Trick-2:-super_fast_cached_generation_mode">
<a class="anchor" href="#Trick-2:-super_fast_cached_generation_mode" aria-hidden="true"><span class="octicon octicon-link"></span></a>Trick 2: <code>super_fast_cached_generation_mode</code><a class="anchor-link" href="#Trick-2:-super_fast_cached_generation_mode"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When I first read the fairseq code, there was a function called <code>make_generation_fast</code> which didnt do much besides catch my eye. What an exciting name! 
Anyways, here is a really slow (pseudocode) way to generate summaries</p>
<div class="highlight"><pre><span></span><span class="n">summary_so_far</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
     <span class="n">encoder_hidden_state</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">article_input_ids</span><span class="p">)</span>
     <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">encoder_hidden_state</span><span class="p">,</span> <span class="n">summary_so_far</span><span class="p">)</span>
     <span class="n">next_word</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
     <span class="n">summary_so_far</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_word</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can just cache the first step and save half the compute</p>
<div class="highlight"><pre><span></span><span class="n">summary_so_far</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">encoder_hidden_state</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">article_input_ids</span><span class="p">)</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
     <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">encoder_hidden_state</span><span class="p">,</span> <span class="n">summary_so_far</span><span class="p">)</span>
     <span class="n">next_word</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
     <span class="n">summary_so_far</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_word</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>but we're not done yet! Remember, inside each layer of <code>model.decoder</code> are two types of attention: while the <code>encoder_attention</code> modules look at <code>encoder_hidden_state</code>, while the <code>self_attention</code> modules look at <code>summary_so_far</code>. For the first type of attention, we can basically cache everything; it's not gunna change during our loop. For self attention, we can cache the keys and values associated with tokens we've already generated, so for each forward pass, all we need to do is compute <code>q</code>, <code>k</code>, and <code>v</code> [^2]</p>
<div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">past_key</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># the seq_len dimension</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">past_value</span><span class="p">,</span> <span class="n">v</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
<p>[^2] lol</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>V1: cache encoder_hidden_state</strong></p>

<pre><code>generated = []
encoder_hidden_state = model.encoder(article_input_ids)
while not Done:
     logits = model.decoder(encoder_hidden_state, generated)

     next_word = logits.argmax()
     generated.append(next_word)</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>V2: cache attention (i,j) for j &lt; t</strong></p>

<pre><code>generated = []
encoder_hidden_state = model.encoder(article_input_ids)
while not Done:
     logits = model.decoder(encoder_hidden_state, generated)

     next_word = logits.argmax()
     generated.append(next_word)</code></pre>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ls</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>2020-02-20-test.ipynb  README.md              <span class="ansi-cyan-intense-fg ansi-bold">my_icons</span>/
2020-03-06-bart.ipynb  gpt2_mask.jpeg
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">article_input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_encode_plus</span><span class="p">([</span><span class="n">SHORTER_ARTICLE</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'pt'</span><span class="p">)[</span><span class="s1">'input_ids'</span><span class="p">]</span>
<span class="n">summary_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">article_input_ids</span><span class="p">,</span> <span class="n">length_penalty</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                             <span class="n">max_length</span><span class="o">=</span><span class="mi">140</span><span class="p">,</span> <span class="n">min_len</span><span class="o">=</span><span class="mi">55</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">summary_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">article_input_ids</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">length_penalty</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                             <span class="n">max_length</span><span class="o">=</span><span class="mi">140</span><span class="p">,</span> <span class="n">min_len</span><span class="o">=</span><span class="mi">55</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">summary_ids</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>"&lt;s&gt;The Palestinian Authority becomes the 123rd member of the International Criminal Court. The move gives the court jurisdiction over alleged crimes in Palestinian territories. Israel and the United States opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki said it was a move toward greater justice."</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Footnotes">
<a class="anchor" href="#Footnotes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Footnotes<a class="anchor-link" href="#Footnotes"> </a>
</h2>
<p>[^2] "Text Summarization with Pretrained Encoders" <a href="https://arxiv.org/abs/1908.08345">https://arxiv.org/abs/1908.08345</a></p>
<p>[^3] Differences between the UniLM Masking strategy and Bart</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Appendix">
<a class="anchor" href="#Appendix" aria-hidden="true"><span class="octicon octicon-link"></span></a>Appendix<a class="anchor-link" href="#Appendix"> </a>
</h3>
<p><img src="./bert_attn_mask.jpg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/fastpages_blog/images/copied_from_nb/./gpt2_mask.jpeg" alt=""> "Left-to-right mask. For a given token in the sequence, we assign a mask value of 0 for this token and the preceding ones; a value of minus infinity for the later ones. This pretty square can also be called and <code>causal_mask</code> autoregressive mask. (Picture Drawn by Remy Louf)"</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Boxes-/-Callouts">
<a class="anchor" href="#Boxes-/-Callouts" aria-hidden="true"><span class="octicon octicon-link"></span></a>Boxes / Callouts<a class="anchor-link" href="#Boxes-/-Callouts"> </a>
</h2>
<p>Typing <code>&gt; Warning: There will be no second warning!</code> will render this:
</p>
<div class="flash flash-error">
    <svg class="octicon octicon-alert octicon octicon-alert" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 000 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 00.01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg>
    <strong>Warning: </strong>There will be no second warning!
</div>
<p>Typing <code>&gt; Important: Pay attention! It's important.</code> will render this:
</p>
<div class="flash flash-warn">
    <svg class="octicon octicon-zap octicon octicon-zap" viewbox="0 0 10 16" version="1.1" width="10" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10 7H6l3-7-9 9h4l-3 7 9-9z"></path></svg>
    <strong>Important: </strong>Pay attention! It’s important.
</div>
<p>Typing <code>&gt; Tip: This is my tip.</code> will render this:
</p>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M16 8.5l-6 6-3-3L8.5 10l1.5 1.5L14.5 7 16 8.5zM5.7 12.2l.8.8H2c-.55 0-1-.45-1-1V3c0-.55.45-1 1-1h7c.55 0 1 .45 1 1v6.5l-.8-.8c-.39-.39-1.03-.39-1.42 0L5.7 10.8a.996.996 0 000 1.41v-.01zM4 4h5V3H4v1zm0 2h5V5H4v1zm0 2h3V7H4v1zM3 9H2v1h1V9zm0-2H2v1h1V7zm0-2H2v1h1V5zm0-2H2v1h1V3z"></path></svg>
    <strong>Tip: </strong>This is my tip.
</div>
<p>Typing <code>&gt; Note: Take note of this.</code> will render this:
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg>
    <strong>Note: </strong>Take note of this.
</div>
<p>Typing <code>&gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine.</code> will render in the docs:
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg>
    <strong>Note: </strong>A doc link to <a href="https://www.fast.ai/">an example website: fast.ai</a> should also work fine.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Footnotes">
<a class="anchor" href="#Footnotes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Footnotes<a class="anchor-link" href="#Footnotes"> </a>
</h2>
<p>You can have footnotes in notebooks just like you can with markdown.</p>
<p>For example, here is a footnote <sup class="footnote-ref" id="fnref-1"><a href="#fn-1">1</a></sup>.</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn-1"><p>This is the footnote.<a href="#fnref-1" class="footnote">↩</a></p></li>
</ol>
</div>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="sshleifer/fastpages_blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/fastpages_blog/jupyter/2020/03/06/bart.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/fastpages_blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/fastpages_blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/fastpages_blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/fastpages_blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/fastpages_blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
